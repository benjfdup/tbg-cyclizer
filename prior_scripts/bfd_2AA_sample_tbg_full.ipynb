{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vvv CHUNK ONLY NEEDED ON JUPYTER vvv ###\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simulate command-line argument for peptide (index 1 in argv)\n",
    "sys.argv = ['notebook', 'QC']  # Replace 'your_peptide_name_here' with the actual peptide name\n",
    "\n",
    "# Now, the code will behave as if it received a command-line argument.\n",
    "peptide = sys.argv[1]  # This works identically to how it does in a .py file\n",
    "\n",
    "### ^^^ CHUNK ONLY NEEDED ON JUPYTER ^^^ ###\n",
    "\n",
    "### vvv ALLOWED PEPTIDES vvv ###\n",
    "# 'IY', 'MN', 'MS', 'LR', 'SW', 'QC', 'SK', 'DQ', 'HH', 'HD', 'DR', 'MQ', 'RD', 'CV', 'EH', 'ES', 'TS', 'PT', 'TR', 'VF', 'GD', 'HV', 'WT', 'NV', 'TG', 'HS', 'RA', 'ID', 'TH', 'TP', 'IS', 'CG', 'IV', 'VI', 'CE', 'LE', 'TW', 'AV', 'NH', 'RK', 'PI', 'KA', 'NT', 'MF', 'DT', 'VP', 'CH', 'SA', 'IL', 'PD', 'NI', 'RH', 'PL', 'IW', 'CM', 'NA', 'HF', 'WC', 'RE', 'HQ', 'IE', 'YS', 'FY', 'SI', 'KL', 'NG', 'VL', 'TM', 'NW', 'MD', 'ND', 'KV', 'CC', 'CF', 'PN', 'PR', 'WY', 'VQ', 'PW', 'RP', 'YW', 'DD', 'MR', 'SM', 'VG', 'LA', 'PK', 'WH', 'FV', 'LC', 'WP', 'EN', 'EG', 'CI', 'WW', 'PM', 'WE', 'VW', 'CD', 'GV', 'TV', 'ML', 'VN', 'GA', 'YN', 'KF', 'DY', 'RW', 'LK', 'PV', 'EF', 'KH', 'LP', 'WA', 'DE', 'MT', 'WN', 'DM', 'TQ', 'IC', 'QP', 'ER', 'VS', 'IH', 'HM', 'QN', 'FL', 'KT', 'PQ', 'IN', 'CQ', 'YC', 'GM', 'CL', 'WR', 'AI', 'LQ', 'RS', 'FW', 'AK', 'VA', 'HL', 'EP', 'GI', 'QH', 'FN', 'CY', 'YF', 'WD', 'VH', 'WS', 'HY', 'GL', 'EM', 'EE', 'TN', 'DS', 'VR', 'LF', 'MM', 'SN', 'VK', 'QE', 'SC', 'NN', 'LH', 'TL', 'II', 'PG', 'AL', 'HE', 'IF', 'FM', 'VC', 'HW', 'YG', 'AE', 'FD', 'YT', 'QL', 'HG', 'CT', 'GK', 'FC', 'RR', 'FK', 'YV', 'AG', 'QR', 'EA', 'SV', 'AQ', 'FI', 'HA', 'KP', 'IA', 'SF', 'FP', 'WG', 'KM', 'LL', 'GY', 'NM', 'DN', 'IP', 'QS', 'SE', 'VT', 'QK', 'EQ', 'AA', 'FG', 'YM', 'NR', 'GF', 'DP', 'VD', 'RM', 'EC', 'LG', 'QT', 'ED', 'AS', 'LI', 'GG', 'MP', 'RI', 'KW', 'FE', 'TC', 'VY', 'SL', 'NS', 'GS', 'AF', 'LS', 'GW', 'YR', 'AY', 'WK', 'LN', 'FT', 'DC', 'PA', 'RN', 'DV', 'NL', 'QY', 'QA', 'PE', 'DI', 'PY', 'RG', 'GH', 'YY', 'FQ', 'DF', 'QV', 'SH', 'CA', 'WM', 'MG', 'VM', 'QD', 'IT', 'SR', 'KK', 'GC', 'YQ', 'DG', 'LD', 'MH', 'WL', 'YP', 'IR', 'YE', 'LT', 'QI', 'HN', 'WV', 'VE', 'GE', 'NP', 'YK', 'PP', 'SG', 'YH', 'FR', 'KY', 'SS', 'SP', 'DA', 'WQ', 'CP', 'EI', 'AW', 'TT', 'QM', 'HR', 'EK', 'YI', 'SY', 'YL', 'MC', 'KC', 'RC', 'CR', 'PF', 'TY', 'DW', 'CW', 'FA', 'AP', 'TD', 'GT', 'TE', 'IM', 'HP', 'EW', 'QW', 'NQ', 'IK', 'KI', 'MA', 'HK', 'AC', 'KR', 'QQ', 'QG', 'CN', 'RT', 'GN', 'AT', 'DH', 'DL', 'CK', 'LM', 'NK', 'NF', 'KD', 'WF', 'KQ', 'RV', 'FS', 'GR', 'MW', 'FF', 'VV', 'MV', 'HC', 'SQ', 'HT', 'LY', 'SD', 'MY', 'IG', 'MI', 'MK', 'GQ', 'PC', 'AR', 'PS', 'LW', 'DK', 'NC', 'EV', 'PH', 'FH', 'WI', 'AD', 'RY', 'KE', 'AM', 'YD', 'AH', 'GP', 'CS', 'IQ', 'KG', 'QF', 'AN', 'LV', 'RQ', 'TA', 'EY', 'KS', 'KN', 'RF', 'RL', 'TK', 'NE', 'ET', 'EL', 'TI', 'TF', 'YA', 'HI', 'NY', 'ST', 'ME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from bgflow.utils import (\n",
    "    as_numpy,\n",
    ")\n",
    "from bgflow import (\n",
    "    DiffEqFlow,\n",
    "    MeanFreeNormalDistribution,\n",
    ")\n",
    "from tbg.models2 import EGNN_dynamics_transferable_MD\n",
    "from bgflow import BlackBoxDynamics\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import mdtraj as md\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path Definitions ###\n",
    "\n",
    "data_path = \"/home/bfd21/rds/hpc-work/2AA-complete\"\n",
    "ecoding_path = \"/home/bfd21/rds/hpc-work/tbg/data/2AA-1-large\"\n",
    "n_dimensions = 3\n",
    "\n",
    "directory = os.fsencode(data_path + \"/val\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "\n",
    "max_atom_number = 0\n",
    "atom_dict = {\"H\": 0, \"C\": 1, \"N\": 2, \"O\": 3, \"S\": 4}\n",
    "scaling = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Validation Peptides ###\n",
    "\n",
    "data_path = \"/home/bfd21/rds/hpc-work/2AA-complete\"\n",
    "ecoding_path = \"/home/bfd21/rds/hpc-work/tbg/data/2AA-1-large\"\n",
    "n_dimensions = 3\n",
    "\n",
    "directory = os.fsencode(data_path + \"/val\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "\n",
    "max_atom_number = 0\n",
    "atom_dict = {\"H\": 0, \"C\": 1, \"N\": 2, \"O\": 3, \"S\": 4}\n",
    "scaling = 30\n",
    "\n",
    "### Initialization of Dictionaries and Variables ###\n",
    "\n",
    "priors = {}\n",
    "topologies = {}\n",
    "atom_types_dict = {}\n",
    "h_dict = {}\n",
    "n_encodings = 76\n",
    "atom_types_ecoding = np.load(\n",
    "    ecoding_path + \"/atom_types_ecoding.npy\", allow_pickle=True\n",
    ").item()\n",
    "\n",
    "amino_dict = {\n",
    "    \"ALA\": 0,\n",
    "    \"ARG\": 1,\n",
    "    \"ASN\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"CYS\": 4,\n",
    "    \"GLN\": 5,\n",
    "    \"GLU\": 6,\n",
    "    \"GLY\": 7,\n",
    "    \"HIS\": 8,\n",
    "    \"ILE\": 9,\n",
    "    \"LEU\": 10,\n",
    "    \"LYS\": 11,\n",
    "    \"MET\": 12,\n",
    "    \"PHE\": 13,\n",
    "    \"PRO\": 14,\n",
    "    \"SER\": 15,\n",
    "    \"THR\": 16,\n",
    "    \"TRP\": 17,\n",
    "    \"TYR\": 18,\n",
    "    \"VAL\": 19,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [00:00<00:00, 128.68it/s]\n"
     ]
    }
   ],
   "source": [
    "### Processing Each Peptide and Creating One-Hot Encodings ###\n",
    "\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/val/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    \n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "\n",
    "            # Handling different atom names\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\"HE\", \"HD\", \"HZ\", \"HH\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "\n",
    "            atom_types.append(atom_name.name)\n",
    "    \n",
    "    # Encoding atoms and amino acids\n",
    "    atom_types_dict[peptide] = np.array([atom_types_ecoding[atom_type] for atom_type in atom_types])\n",
    "    \n",
    "    atom_onehot = torch.nn.functional.one_hot(torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding))\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(torch.tensor(amino_idx), num_classes=2)\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(torch.tensor(amino_types), num_classes=20)\n",
    "\n",
    "    # Concatenating encodings\n",
    "    h_dict[peptide] = torch.cat([amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1)\n",
    "\n",
    "    # Setting up priors\n",
    "    priors[peptide] = MeanFreeNormalDistribution(n_atoms * n_dimensions, n_atoms, two_event_dims=False).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 723.94it/s]\n"
     ]
    }
   ],
   "source": [
    "### Train Peptides (Similar Logic as Validation Peptides) ###\n",
    "\n",
    "directory = os.fsencode(data_path + \"/train\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/train/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 699.97it/s]\n"
     ]
    }
   ],
   "source": [
    "### Test Peptides (Similar Logic as Validation Peptides) ###\n",
    "\n",
    "directory = os.fsencode(data_path + \"/test\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/test/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model tbg_full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3607489/1150278574.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH_last)\n"
     ]
    }
   ],
   "source": [
    "### Defining and Loading the EGNN Dynamics with Brute Force Estimation ###\n",
    "\n",
    "max_atom_number = 51\n",
    "\n",
    "peptide = sys.argv[1]\n",
    "\n",
    "\n",
    "class BruteForceEstimatorFast(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Exact bruteforce estimation of the divergence of a dynamics function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, dynamics, t, xs):\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            xs.requires_grad_(True)\n",
    "            x = [xs[:, [i]] for i in range(xs.size(1))]\n",
    "\n",
    "            dxs = dynamics(t, torch.cat(x, dim=1))\n",
    "\n",
    "            assert len(dxs.shape) == 2, \"`dxs` must have shape [n_btach, system_dim]\"\n",
    "            divergence = 0\n",
    "            for i in range(xs.size(1)):\n",
    "                divergence += torch.autograd.grad(\n",
    "                    dxs[:, [i]], x[i], torch.ones_like(dxs[:, [i]]), retain_graph=True\n",
    "                )[0]\n",
    "\n",
    "        return dxs, -divergence.view(-1, 1)\n",
    "\n",
    "\n",
    "hidden_nf = 128\n",
    "n_layers = 9\n",
    "net_dynamics = EGNN_dynamics_transferable_MD(\n",
    "    n_particles=max_atom_number,\n",
    "    h_size=n_encodings,\n",
    "    device=\"cuda\",\n",
    "    n_dimension=n_dimensions,\n",
    "    hidden_nf=hidden_nf,\n",
    "    act_fn=torch.nn.SiLU(),\n",
    "    n_layers=n_layers,\n",
    "    recurrent=True,\n",
    "    tanh=True,\n",
    "    attention=True,\n",
    "    condition_time=True,\n",
    "    mode=\"egnn_dynamics\",\n",
    "    agg=\"sum\",\n",
    ")\n",
    "\n",
    "bb_dynamics = BlackBoxDynamics(\n",
    "    dynamics_function=net_dynamics, divergence_estimator=BruteForceEstimatorFast()\n",
    ")\n",
    "\n",
    "flow = DiffEqFlow(dynamics=bb_dynamics)\n",
    "\n",
    "filename = f\"tbg_full\"\n",
    "\n",
    "PATH_last = f\"models/{filename}\"\n",
    "checkpoint = torch.load(PATH_last)\n",
    "flow.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_epoch = checkpoint[\"epoch\"]\n",
    "global_it = checkpoint[\"global_it\"]\n",
    "print(f\"Successfully loaded model {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wrapper for Masking and Handling Particles in EGNN Dynamics ###\n",
    "\n",
    "class NetDynamicsWrapper(torch.nn.Module):\n",
    "    def __init__(self, net_dynamics, n_particles, max_n_particles, h_initial):\n",
    "        super().__init__()\n",
    "        self.net_dynamics = net_dynamics\n",
    "        self.n_particles = n_particles\n",
    "        mask = torch.ones((1, n_particles))\n",
    "        mask = torch.nn.functional.pad(\n",
    "            mask, (0, (max_n_particles - n_particles))\n",
    "        )  # .bool()\n",
    "        edge_mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        # mask diagonal\n",
    "        diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "        edge_mask *= diag_mask\n",
    "        self.node_mask = mask\n",
    "        self.edge_mask = edge_mask\n",
    "        self.h_initial = torch.cat(\n",
    "            [h_initial, torch.zeros(max_n_particles - n_particles, h_initial.size(1))]\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def forward(self, t, xs, args=None):\n",
    "        n_batch = xs.size(0)\n",
    "        node_mask = self.node_mask.repeat(n_batch, 1).to(xs)\n",
    "        edge_mask = self.edge_mask.repeat(n_batch, 1, 1).to(xs)\n",
    "        h_initial = self.h_initial.repeat(n_batch, 1, 1).to(xs)\n",
    "        return self.net_dynamics(\n",
    "            t, xs, h_initial, node_mask=node_mask, edge_mask=edge_mask\n",
    "        )\n",
    "\n",
    "\n",
    "net_dynamics_wrapper = NetDynamicsWrapper(\n",
    "    net_dynamics,\n",
    "    n_particles=len(h_dict[peptide]),\n",
    "    max_n_particles=max_atom_number,\n",
    "    h_initial=h_dict[peptide],\n",
    ")\n",
    "\n",
    "\n",
    "flow._dynamics._dynamics._dynamics_function = net_dynamics_wrapper\n",
    "flow._integrator_atol = 1e-4\n",
    "flow._integrator_rtol = 1e-4\n",
    "flow._use_checkpoints = False\n",
    "flow._kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start new sampling\n",
      "Sampling with dlogp\n",
      "QC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0246,  1.1751, -0.1144,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4431,  1.5776,  0.8194,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.2467,  0.6051, -2.3616,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.2236, -0.6155,  0.8282,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 2.1624,  0.4491,  1.1531,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3182,  0.4643, -1.0462,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0')\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Setup Variables and Parameters ###\n",
    "n_samples = 500\n",
    "n_sample_batches = 200\n",
    "\n",
    "dim = len(h_dict[peptide]) * 3  # Peptide dimension (3D)\n",
    "with_dlogp = True  # Flag for sampling with dlogp (log probability)\n",
    "\n",
    "\n",
    "### Loading Precomputed Samples (if available) ###\n",
    "\n",
    "if with_dlogp:\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        dlogp_np = npz[\"dlogp_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "        dlogp_np = np.empty(shape=(0))\n",
    "\n",
    "    print(\"Sampling with dlogp\")\n",
    "    print(peptide)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            \n",
    "            samples, dlogp = flow(latent)\n",
    "\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, samples[:, :dim].detach().cpu().numpy())\n",
    "\n",
    "            dlogp_np = np.append(dlogp_np, as_numpy(dlogp))\n",
    "\n",
    "        # print(i)\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "            dlogp_np=dlogp_np,\n",
    "        )\n",
    "else:\n",
    "    n_samples *= 10 # Increase number of samples if not using dlogp\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}_nologp.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "    print(\"Sampling without dlogp\")\n",
    "    from torchdyn.core import NeuralODE\n",
    "\n",
    "    node = NeuralODE(\n",
    "        net_dynamics_wrapper,\n",
    "        solver=\"dopri5\",\n",
    "        sensitivity=\"adjoint\",\n",
    "        atol=1e-4,\n",
    "        rtol=1e-4,\n",
    "    )\n",
    "    t_span = torch.linspace(0, 1, 100)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            traj = node.trajectory(\n",
    "                latent,\n",
    "                t_span=t_span,\n",
    "            )\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, as_numpy(traj[-1])[:, :dim])\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}_nologp\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries and Setup Data ###\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from bgflow.utils import (\n",
    "    as_numpy,\n",
    ")\n",
    "from bgflow import (\n",
    "    DiffEqFlow,\n",
    "    MeanFreeNormalDistribution,\n",
    ")\n",
    "from tbg.models2 import EGNN_dynamics_transferable_MD\n",
    "from bgflow import BlackBoxDynamics\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import mdtraj as md\n",
    "import sys\n",
    "\n",
    "data_path = \"data/2AA-1-large\"\n",
    "n_dimensions = 3\n",
    "\n",
    "directory = os.fsencode(data_path + \"/val\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "\n",
    "max_atom_number = 0\n",
    "atom_dict = {\"H\": 0, \"C\": 1, \"N\": 2, \"O\": 3, \"S\": 4}\n",
    "scaling = 30\n",
    "\n",
    "priors = {}\n",
    "topologies = {}\n",
    "atom_types_dict = {}\n",
    "h_dict = {}\n",
    "n_encodings = 76\n",
    "atom_types_ecoding = np.load(\n",
    "    data_path + \"/atom_types_ecoding.npy\", allow_pickle=True\n",
    ").item()\n",
    "\n",
    "amino_dict = {\n",
    "    \"ALA\": 0,\n",
    "    \"ARG\": 1,\n",
    "    \"ASN\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"CYS\": 4,\n",
    "    \"GLN\": 5,\n",
    "    \"GLU\": 6,\n",
    "    \"GLY\": 7,\n",
    "    \"HIS\": 8,\n",
    "    \"ILE\": 9,\n",
    "    \"LEU\": 10,\n",
    "    \"LYS\": 11,\n",
    "    \"MET\": 12,\n",
    "    \"PHE\": 13,\n",
    "    \"PRO\": 14,\n",
    "    \"SER\": 15,\n",
    "    \"THR\": 16,\n",
    "    \"TRP\": 17,\n",
    "    \"TYR\": 18,\n",
    "    \"VAL\": 19,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading and Encoding Peptides ###\n",
    "\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/val/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:00<00:00, 733.02it/s]\n"
     ]
    }
   ],
   "source": [
    "### Train chunk ###\n",
    "\n",
    "directory = os.fsencode(data_path + \"/train\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/train/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 712.48it/s]\n"
     ]
    }
   ],
   "source": [
    "### Test chunk ###\n",
    "\n",
    "directory = os.fsencode(data_path + \"/test\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/test/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BruteForceEstimatorFast Class ###\n",
    "\n",
    "max_atom_number = 51\n",
    "\n",
    "peptide = sys.argv[1]\n",
    "\n",
    "\n",
    "class BruteForceEstimatorFast(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Exact bruteforce estimation of the divergence of a dynamics function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, dynamics, t, xs):\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            xs.requires_grad_(True)\n",
    "            x = [xs[:, [i]] for i in range(xs.size(1))]\n",
    "\n",
    "            dxs = dynamics(t, torch.cat(x, dim=1))\n",
    "\n",
    "            assert len(dxs.shape) == 2, \"`dxs` must have shape [n_btach, system_dim]\"\n",
    "            divergence = 0\n",
    "            for i in range(xs.size(1)):\n",
    "                divergence += torch.autograd.grad(\n",
    "                    dxs[:, [i]], x[i], torch.ones_like(dxs[:, [i]]), retain_graph=True\n",
    "                )[0]\n",
    "\n",
    "        return dxs, -divergence.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Dynamics and Load Model Checkpoint ###\n",
    "\n",
    "hidden_nf = 128\n",
    "n_layers = 9\n",
    "net_dynamics = EGNN_dynamics_transferable_MD(\n",
    "    n_particles=max_atom_number,\n",
    "    h_size=n_encodings,\n",
    "    device=\"cuda\",\n",
    "    n_dimension=n_dimensions,\n",
    "    hidden_nf=hidden_nf,\n",
    "    act_fn=torch.nn.SiLU(),\n",
    "    n_layers=n_layers,\n",
    "    recurrent=True,\n",
    "    tanh=True,\n",
    "    attention=True,\n",
    "    condition_time=True,\n",
    "    mode=\"egnn_dynamics\",\n",
    "    agg=\"sum\",\n",
    ")\n",
    "\n",
    "bb_dynamics = BlackBoxDynamics(\n",
    "    dynamics_function=net_dynamics, divergence_estimator=BruteForceEstimatorFast()\n",
    ")\n",
    "\n",
    "flow = DiffEqFlow(dynamics=bb_dynamics)\n",
    "\n",
    "filename = f\"tbg_full\"\n",
    "\n",
    "PATH_last = f\"models/{filename}\"\n",
    "checkpoint = torch.load(PATH_last)\n",
    "flow.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_epoch = checkpoint[\"epoch\"]\n",
    "global_it = checkpoint[\"global_it\"]\n",
    "print(f\"Successfully loaded model {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NetDynamicsWrapper Class ###\n",
    "\n",
    "class NetDynamicsWrapper(torch.nn.Module):\n",
    "    def __init__(self, net_dynamics, n_particles, max_n_particles, h_initial):\n",
    "        super().__init__()\n",
    "        self.net_dynamics = net_dynamics\n",
    "        self.n_particles = n_particles\n",
    "        mask = torch.ones((1, n_particles))\n",
    "        mask = torch.nn.functional.pad(\n",
    "            mask, (0, (max_n_particles - n_particles))\n",
    "        )  # .bool()\n",
    "        edge_mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        # mask diagonal\n",
    "        diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "        edge_mask *= diag_mask\n",
    "        self.node_mask = mask\n",
    "        self.edge_mask = edge_mask\n",
    "        self.h_initial = torch.cat(\n",
    "            [h_initial, torch.zeros(max_n_particles - n_particles, h_initial.size(1))]\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def forward(self, t, xs, args=None):\n",
    "        n_batch = xs.size(0)\n",
    "        node_mask = self.node_mask.repeat(n_batch, 1).to(xs)\n",
    "        edge_mask = self.edge_mask.repeat(n_batch, 1, 1).to(xs)\n",
    "        h_initial = self.h_initial.repeat(n_batch, 1, 1).to(xs)\n",
    "        return self.net_dynamics(\n",
    "            t, xs, h_initial, node_mask=node_mask, edge_mask=edge_mask\n",
    "        )\n",
    "    \n",
    "### Set up Dynamics Function in the Flow Model ###\n",
    "\n",
    "net_dynamics_wrapper = NetDynamicsWrapper(\n",
    "    net_dynamics,\n",
    "    n_particles=len(h_dict[peptide]),\n",
    "    max_n_particles=max_atom_number,\n",
    "    h_initial=h_dict[peptide],\n",
    ")\n",
    "flow._dynamics._dynamics._dynamics_function = net_dynamics_wrapper\n",
    "\n",
    "flow._integrator_atol = 1e-4\n",
    "flow._integrator_rtol = 1e-4\n",
    "flow._use_checkpoints = False\n",
    "flow._kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sampling Logic ###\n",
    "\n",
    "n_samples = 500\n",
    "n_sample_batches = 200\n",
    "\n",
    "dim = len(h_dict[peptide]) * 3\n",
    "with_dlogp = True\n",
    "\n",
    "if with_dlogp:\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        dlogp_np = npz[\"dlogp_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "        dlogp_np = np.empty(shape=(0))\n",
    "\n",
    "    print(\"Sampling with dlogp\")\n",
    "    print(peptide)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            samples, dlogp = flow(latent)\n",
    "\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, samples[:, :dim].detach().cpu().numpy())\n",
    "\n",
    "            dlogp_np = np.append(dlogp_np, as_numpy(dlogp))\n",
    "\n",
    "        # print(i)\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "            dlogp_np=dlogp_np,\n",
    "        )\n",
    "else:\n",
    "    n_samples *= 10\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}_nologp.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "    print(\"Sampling without dlogp\")\n",
    "    from torchdyn.core import NeuralODE\n",
    "\n",
    "    node = NeuralODE(\n",
    "        net_dynamics_wrapper,\n",
    "        solver=\"dopri5\",\n",
    "        sensitivity=\"adjoint\",\n",
    "        atol=1e-4,\n",
    "        rtol=1e-4,\n",
    "    )\n",
    "    t_span = torch.linspace(0, 1, 100)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            traj = node.trajectory(\n",
    "                latent,\n",
    "                t_span=t_span,\n",
    "            )\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, as_numpy(traj[-1])[:, :dim])\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}_nologp\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start new sampling\n",
      "Sampling with dlogp\n",
      "QC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/rds/user/bfd21/hpc-work/tbg/tbg/models2.py:208: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).to(xs)\n",
      "  0%|          | 0/200 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m latent \u001b[38;5;241m=\u001b[39m priors[peptide]\u001b[38;5;241m.\u001b[39msample(n_samples)\n\u001b[1;32m     25\u001b[0m latent \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m     26\u001b[0m     latent, (\u001b[38;5;241m0\u001b[39m, (max_atom_number \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(h_dict[peptide])) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m samples, dlogp \u001b[38;5;241m=\u001b[39m \u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m latent_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(latent_np, latent[:, :dim]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     31\u001b[0m samples_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(samples_np, samples[:, :dim]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/bgflow-0.3.0+19.gfbba56f-py3.8.egg/bgflow/nn/flow/base.py:33\u001b[0m, in \u001b[0;36mFlow.forward\u001b[0;34m(self, inverse, *xs, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverse(\u001b[38;5;241m*\u001b[39mxs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/bgflow-0.3.0+19.gfbba56f-py3.8.egg/bgflow/nn/flow/diffeq.py:46\u001b[0m, in \u001b[0;36mDiffEqFlow._forward\u001b[0;34m(self, *xs, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mxs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/bgflow-0.3.0+19.gfbba56f-py3.8.egg/bgflow/nn/flow/diffeq.py:78\u001b[0m, in \u001b[0;36mDiffEqFlow._run_ode\u001b[0;34m(self, dynamics, *xs, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_checkpoints:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdiffeq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m odeint_adjoint\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;241m*\u001b[39mys, dlogp \u001b[38;5;241m=\u001b[39m \u001b[43modeint_adjoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_integrator_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_integrator_rtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_integrator_atol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     ys \u001b[38;5;241m=\u001b[39m [y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py:198\u001b[0m, in \u001b[0;36modeint_adjoint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, adjoint_params)\u001b[0m\n\u001b[1;32m    195\u001b[0m state_norm \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    196\u001b[0m handle_adjoint_norm_(adjoint_options, shapes, state_norm)\n\u001b[0;32m--> 198\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mOdeintAdjointMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_rtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_atol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                \u001b[49m\u001b[43madjoint_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjoint_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madjoint_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     solution \u001b[38;5;241m=\u001b[39m ans\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/adjoint.py:25\u001b[0m, in \u001b[0;36mOdeintAdjointMethod.forward\u001b[0;34m(ctx, shapes, func, y0, t, rtol, atol, method, options, event_fn, adjoint_rtol, adjoint_atol, adjoint_method, adjoint_options, t_requires_grad, *adjoint_params)\u001b[0m\n\u001b[1;32m     22\u001b[0m ctx\u001b[38;5;241m.\u001b[39mevent_mode \u001b[38;5;241m=\u001b[39m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         y \u001b[38;5;241m=\u001b[39m ans\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[38;5;241m=\u001b[39m SOLVERS[method](func\u001b[38;5;241m=\u001b[39mfunc, y0\u001b[38;5;241m=\u001b[39my0, rtol\u001b[38;5;241m=\u001b[39mrtol, atol\u001b[38;5;241m=\u001b[39matol, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mintegrate_until_event(t[\u001b[38;5;241m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/solvers.py:28\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m solution[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0\n\u001b[1;32m     27\u001b[0m t \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_before_integrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[1;32m     30\u001b[0m     solution[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_advance(t[i])\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/rk_common.py:161\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._before_integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_before_integrate\u001b[39m(\u001b[38;5;28mself\u001b[39m, t):\n\u001b[1;32m    160\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 161\u001b[0m     f0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m         first_step \u001b[38;5;241m=\u001b[39m _select_initial_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, t[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my0, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrtol, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matol,\n\u001b[1;32m    164\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm, f0\u001b[38;5;241m=\u001b[39mf0)\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torchdiffeq/_impl/misc.py:138\u001b[0m, in \u001b[0;36m_TupleFunc.forward\u001b[0;34m(self, t, y)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t, y):\n\u001b[0;32m--> 138\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_flat_to_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([f_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f_ \u001b[38;5;129;01min\u001b[39;00m f])\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/bgflow-0.3.0+19.gfbba56f-py3.8.egg/bgflow/nn/flow/dynamics/density.py:37\u001b[0m, in \u001b[0;36mDensityDynamics.forward\u001b[0;34m(self, t, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mComputes the change of the system `dx/dt` at state `x` and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mtime `t`. Furthermore, computes the change of density, happening\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    (`dxs`) and the update of the log density (`dlogp`).\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m xs \u001b[38;5;241m=\u001b[39m state[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;241m*\u001b[39mdxs, dlogp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m*\u001b[39mdxs, \u001b[38;5;241m-\u001b[39mdlogp)\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/bgflow-0.3.0+19.gfbba56f-py3.8.egg/bgflow/nn/flow/dynamics/blackbox.py:36\u001b[0m, in \u001b[0;36mBlackBoxDynamics.forward\u001b[0;34m(self, t, *xs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mComputes the change of the system `dxs` at state `xs` and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mtime `t`. Furthermore, can also compute the change of log density\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    (`dxs`) and the update of the log density (`dlogp`)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_divergence:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;241m*\u001b[39mdxs, divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divergence_estimator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamics_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     dxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamics_function(t, xs)\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rds/hpc-work/miniconda3/envs/test_ben0/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mBruteForceEstimatorFast.forward\u001b[0;34m(self, dynamics, t, xs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(xs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     27\u001b[0m         divergence \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[0;32m---> 28\u001b[0m             \u001b[43mdxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, x[i], torch\u001b[38;5;241m.\u001b[39mones_like(dxs[:, [i]]), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dxs, \u001b[38;5;241m-\u001b[39mdivergence\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from bgflow.utils import (\n",
    "    as_numpy,\n",
    ")\n",
    "from bgflow import (\n",
    "    DiffEqFlow,\n",
    "    MeanFreeNormalDistribution,\n",
    ")\n",
    "from tbg.models2 import EGNN_dynamics_transferable_MD\n",
    "from bgflow import BlackBoxDynamics\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import mdtraj as md\n",
    "import sys\n",
    "\n",
    "\n",
    "data_path = \"data/2AA-1-large\"\n",
    "n_dimensions = 3\n",
    "\n",
    "directory = os.fsencode(data_path + \"/val\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "\n",
    "max_atom_number = 0\n",
    "atom_dict = {\"H\": 0, \"C\": 1, \"N\": 2, \"O\": 3, \"S\": 4}\n",
    "scaling = 30\n",
    "\n",
    "priors = {}\n",
    "topologies = {}\n",
    "atom_types_dict = {}\n",
    "h_dict = {}\n",
    "n_encodings = 76\n",
    "atom_types_ecoding = np.load(\n",
    "    data_path + \"/atom_types_ecoding.npy\", allow_pickle=True\n",
    ").item()\n",
    "\n",
    "amino_dict = {\n",
    "    \"ALA\": 0,\n",
    "    \"ARG\": 1,\n",
    "    \"ASN\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"CYS\": 4,\n",
    "    \"GLN\": 5,\n",
    "    \"GLU\": 6,\n",
    "    \"GLY\": 7,\n",
    "    \"HIS\": 8,\n",
    "    \"ILE\": 9,\n",
    "    \"LEU\": 10,\n",
    "    \"LYS\": 11,\n",
    "    \"MET\": 12,\n",
    "    \"PHE\": 13,\n",
    "    \"PRO\": 14,\n",
    "    \"SER\": 15,\n",
    "    \"THR\": 16,\n",
    "    \"TRP\": 17,\n",
    "    \"TYR\": 18,\n",
    "    \"VAL\": 19,\n",
    "}\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/val/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()\n",
    "\n",
    "directory = os.fsencode(data_path + \"/train\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/train/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()\n",
    "\n",
    "directory = os.fsencode(data_path + \"/test\")\n",
    "validation_peptides = []\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".pdb\"):\n",
    "        validation_peptides.append(filename[:2])\n",
    "for peptide in tqdm.tqdm(validation_peptides):\n",
    "\n",
    "    topologies[peptide] = md.load_topology(\n",
    "        data_path + f\"/test/{peptide}-traj-state0.pdb\"\n",
    "    )\n",
    "    n_atoms = len(list(topologies[peptide].atoms))\n",
    "    atom_types = []\n",
    "    atom_types = []\n",
    "    amino_idx = []\n",
    "    amino_types = []\n",
    "    for i, amino in enumerate(topologies[peptide].residues):\n",
    "\n",
    "        for atom_name in amino.atoms:\n",
    "            amino_idx.append(i)\n",
    "            amino_types.append(amino_dict[amino.name])\n",
    "            if atom_name.name[0] == \"H\" and atom_name.name[-1] in (\"1\", \"2\", \"3\"):\n",
    "                if amino_dict[amino.name] in (8, 13, 17, 18) and atom_name.name[:2] in (\n",
    "                    \"HE\",\n",
    "                    \"HD\",\n",
    "                    \"HZ\",\n",
    "                    \"HH\",\n",
    "                ):\n",
    "                    pass\n",
    "                else:\n",
    "                    atom_name.name = atom_name.name[:-1]\n",
    "            if atom_name.name[:2] == \"OE\" or atom_name.name[:2] == \"OD\":\n",
    "                atom_name.name = atom_name.name[:-1]\n",
    "            atom_types.append(atom_name.name)\n",
    "    atom_types_dict[peptide] = np.array(\n",
    "        [atom_types_ecoding[atom_type] for atom_type in atom_types]\n",
    "    )\n",
    "    atom_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(atom_types_dict[peptide]), num_classes=len(atom_types_ecoding)\n",
    "    )\n",
    "    amino_idx_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_idx), num_classes=2\n",
    "    )\n",
    "    amino_types_onehot = torch.nn.functional.one_hot(\n",
    "        torch.tensor(amino_types), num_classes=20\n",
    "    )\n",
    "\n",
    "    h_dict[peptide] = torch.cat(\n",
    "        [amino_idx_onehot, amino_types_onehot, atom_onehot], dim=1\n",
    "    )\n",
    "    priors[peptide] = MeanFreeNormalDistribution(\n",
    "        n_atoms * n_dimensions, n_atoms, two_event_dims=False\n",
    "    ).cuda()\n",
    "max_atom_number = 51\n",
    "\n",
    "peptide = sys.argv[1]\n",
    "\n",
    "\n",
    "class BruteForceEstimatorFast(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Exact bruteforce estimation of the divergence of a dynamics function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, dynamics, t, xs):\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            xs.requires_grad_(True)\n",
    "            x = [xs[:, [i]] for i in range(xs.size(1))]\n",
    "\n",
    "            dxs = dynamics(t, torch.cat(x, dim=1))\n",
    "\n",
    "            assert len(dxs.shape) == 2, \"`dxs` must have shape [n_btach, system_dim]\"\n",
    "            divergence = 0\n",
    "            for i in range(xs.size(1)):\n",
    "                divergence += torch.autograd.grad(\n",
    "                    dxs[:, [i]], x[i], torch.ones_like(dxs[:, [i]]), retain_graph=True\n",
    "                )[0]\n",
    "\n",
    "        return dxs, -divergence.view(-1, 1)\n",
    "\n",
    "\n",
    "hidden_nf = 128\n",
    "n_layers = 9\n",
    "net_dynamics = EGNN_dynamics_transferable_MD(\n",
    "    n_particles=max_atom_number,\n",
    "    h_size=n_encodings,\n",
    "    device=\"cuda\",\n",
    "    n_dimension=n_dimensions,\n",
    "    hidden_nf=hidden_nf,\n",
    "    act_fn=torch.nn.SiLU(),\n",
    "    n_layers=n_layers,\n",
    "    recurrent=True,\n",
    "    tanh=True,\n",
    "    attention=True,\n",
    "    condition_time=True,\n",
    "    mode=\"egnn_dynamics\",\n",
    "    agg=\"sum\",\n",
    ")\n",
    "\n",
    "bb_dynamics = BlackBoxDynamics(\n",
    "    dynamics_function=net_dynamics, divergence_estimator=BruteForceEstimatorFast()\n",
    ")\n",
    "\n",
    "flow = DiffEqFlow(dynamics=bb_dynamics)\n",
    "\n",
    "filename = f\"tbg_full\"\n",
    "\n",
    "PATH_last = f\"models/{filename}\"\n",
    "checkpoint = torch.load(PATH_last)\n",
    "flow.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "loaded_epoch = checkpoint[\"epoch\"]\n",
    "global_it = checkpoint[\"global_it\"]\n",
    "print(f\"Successfully loaded model {filename}\")\n",
    "\n",
    "\n",
    "class NetDynamicsWrapper(torch.nn.Module):\n",
    "    def __init__(self, net_dynamics, n_particles, max_n_particles, h_initial):\n",
    "        super().__init__()\n",
    "        self.net_dynamics = net_dynamics\n",
    "        self.n_particles = n_particles\n",
    "        mask = torch.ones((1, n_particles))\n",
    "        mask = torch.nn.functional.pad(\n",
    "            mask, (0, (max_n_particles - n_particles))\n",
    "        )  # .bool()\n",
    "        edge_mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "        # mask diagonal\n",
    "        diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "        edge_mask *= diag_mask\n",
    "        self.node_mask = mask\n",
    "        self.edge_mask = edge_mask\n",
    "        self.h_initial = torch.cat(\n",
    "            [h_initial, torch.zeros(max_n_particles - n_particles, h_initial.size(1))]\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "    def forward(self, t, xs, args=None):\n",
    "        n_batch = xs.size(0)\n",
    "        node_mask = self.node_mask.repeat(n_batch, 1).to(xs)\n",
    "        edge_mask = self.edge_mask.repeat(n_batch, 1, 1).to(xs)\n",
    "        h_initial = self.h_initial.repeat(n_batch, 1, 1).to(xs)\n",
    "        return self.net_dynamics(\n",
    "            t, xs, h_initial, node_mask=node_mask, edge_mask=edge_mask\n",
    "        )\n",
    "\n",
    "\n",
    "net_dynamics_wrapper = NetDynamicsWrapper(\n",
    "    net_dynamics,\n",
    "    n_particles=len(h_dict[peptide]),\n",
    "    max_n_particles=max_atom_number,\n",
    "    h_initial=h_dict[peptide],\n",
    ")\n",
    "flow._dynamics._dynamics._dynamics_function = net_dynamics_wrapper\n",
    "\n",
    "flow._integrator_atol = 1e-4\n",
    "flow._integrator_rtol = 1e-4\n",
    "flow._use_checkpoints = False\n",
    "flow._kwargs = {}\n",
    "\n",
    "n_samples = 500\n",
    "n_sample_batches = 200\n",
    "\n",
    "dim = len(h_dict[peptide]) * 3\n",
    "with_dlogp = True\n",
    "\n",
    "if with_dlogp:\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        dlogp_np = npz[\"dlogp_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "        dlogp_np = np.empty(shape=(0))\n",
    "\n",
    "    print(\"Sampling with dlogp\")\n",
    "    print(peptide)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            samples, dlogp = flow(latent)\n",
    "\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, samples[:, :dim].detach().cpu().numpy())\n",
    "\n",
    "            dlogp_np = np.append(dlogp_np, as_numpy(dlogp))\n",
    "\n",
    "        # print(i)\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "            dlogp_np=dlogp_np,\n",
    "        )\n",
    "else:\n",
    "    n_samples *= 10\n",
    "    try:\n",
    "        npz = np.load(f\"result_data/{filename}_{peptide}_nologp.npz\")\n",
    "        latent_np = npz[\"latent_np\"]\n",
    "        samples_np = npz[\"samples_np\"]\n",
    "        print(\"Successfully loaded samples\")\n",
    "    except:\n",
    "        print(\"Start new sampling\")\n",
    "        latent_np = np.empty(shape=(0))\n",
    "        samples_np = np.empty(shape=(0))\n",
    "    print(\"Sampling without dlogp\")\n",
    "    from torchdyn.core import NeuralODE\n",
    "\n",
    "    node = NeuralODE(\n",
    "        net_dynamics_wrapper,\n",
    "        solver=\"dopri5\",\n",
    "        sensitivity=\"adjoint\",\n",
    "        atol=1e-4,\n",
    "        rtol=1e-4,\n",
    "    )\n",
    "    t_span = torch.linspace(0, 1, 100)\n",
    "    for i in tqdm.tqdm(range(n_sample_batches)):\n",
    "        with torch.no_grad():\n",
    "            latent = priors[peptide].sample(n_samples)\n",
    "            latent = torch.nn.functional.pad(\n",
    "                latent, (0, (max_atom_number - len(h_dict[peptide])) * 3)\n",
    "            )\n",
    "            traj = node.trajectory(\n",
    "                latent,\n",
    "                t_span=t_span,\n",
    "            )\n",
    "            latent_np = np.append(latent_np, latent[:, :dim].detach().cpu().numpy())\n",
    "            samples_np = np.append(samples_np, as_numpy(traj[-1])[:, :dim])\n",
    "        np.savez(\n",
    "            f\"result_data/{filename}_{peptide}_nologp\",\n",
    "            latent_np=latent_np.reshape(-1, dim),\n",
    "            samples_np=samples_np.reshape(-1, dim),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_ben0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
